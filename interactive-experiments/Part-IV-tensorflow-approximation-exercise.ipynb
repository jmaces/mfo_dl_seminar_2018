{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximating functions with deep ReLU networks\n",
    "*Practical session during the [Oberwolfach Seminar on Mathematics of Deep Learning](https://www.mfo.de/occasion/1842b), October, 2018.*\n",
    "The content is mostly based on [D. Yarotsky, 2017](https://www.sciencedirect.com/science/article/pii/S0893608017301545).\n",
    "\n",
    "## Part IV: Approximating functions with Tensorflow networks\n",
    "\n",
    "Let us now realize our approximation theoretic thoughts in Tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import target_functions as tfunc\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (15,6)\n",
    "plt.rcParams['text.usetex'] = True\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Setup\n",
    "\n",
    "Lets start with defining some constants, settings, configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general settings\n",
    "DOMAIN = [0.0, 1.0, 0.0, 1.0]   # unit square [0, 1] x [0, 1]\n",
    "RESOLUTION = 200                # grid resolution for testing the trained model\n",
    "\n",
    "# network settings\n",
    "WIDTH = 20              # number of neurons per hidden layer\n",
    "DEPTH = 6               # total number of layers (number of hidden layers is DEPTH-2)\n",
    "ACTIVATION = tf.nn.relu # hidden layer activation function\n",
    "\n",
    "# training settings\n",
    "INIT_L_RATE  = 2e-1\n",
    "FINAL_L_RATE = 1e-3\n",
    "NUM_ITER     = 8000\n",
    "BATCH_SIZE   = 2048\n",
    "\n",
    "# setup target function\n",
    "REGULARITY = [2, 3]\n",
    "def target_func(x, y):\n",
    "    smooth_part = 2*tfunc.bernstein2d(x, y, [2, 3], [3, 4])\n",
    "    nonsmooth_part = 2*np.math.factorial(REGULARITY[0]) \\\n",
    "        * np.math.factorial(REGULARITY[1]) \\\n",
    "        * tfunc.signpoly2d(x-0.4, y-0.6, REGULARITY)\n",
    "    return smooth_part + nonsmooth_part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model\n",
    "\n",
    "Next we build the neural network computational graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define input and target placeholders\n",
    "##### ##### COMPLETE THE CODE HERE  ##### #####\n",
    "'''\n",
    " Define tensors for the input and target placeholders here.\n",
    " The input should accept feeds of batches of vectors with two components (x and y coordinates).\n",
    " The target should accept feeds of batches of vectors with one component (the target function values).\n",
    " \n",
    " We suggest to use the names 'input' and 'target' for these tensors. If you choose different names you might\n",
    " have to adjust something in other parts of the code accordingly.\n",
    "'''\n",
    "##### ##### ##### ##### ##### ##### ##### #####\n",
    "\n",
    "# define hidden layers with forward skip connections\n",
    "hidden = (DEPTH-1)*[None]\n",
    "hidden[0] = input\n",
    "for l in range(DEPTH-2):\n",
    "##### ##### COMPLETE THE CODE HERE  ##### #####\n",
    "'''\n",
    " Define operations for the hidden layers. Use WIDTH for the number of neurons per hidden layer and ACTIVATION\n",
    " for the activation function. Note that we have already put in a loop over the depth of the network, so within the\n",
    " loop we only need to specify the operation of a single fully connected layer.\n",
    " \n",
    " Do not forget about the skip connections in the network architecture used by Yarotsky.\n",
    "'''\n",
    "##### ##### ##### ##### ##### ##### ##### #####\n",
    "\n",
    "\n",
    "# final layer without ReLU activation\n",
    "##### ##### COMPLETE THE CODE HERE  ##### #####\n",
    "'''\n",
    " Define operations for the last (output) layer. Remember that the final output layer has only one neuron\n",
    " and uses no activation function.\n",
    " \n",
    " We suggest to use the name 'prediction' for this tensor. If you choose a different name you might\n",
    " have to adjust something in other parts of the code accordingly.\n",
    "'''\n",
    "##### ##### ##### ##### ##### ##### ##### #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the training procedure\n",
    "\n",
    "Next we define the loss, optimizer, descent steps, and other variable we want to keep track of during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use decaying learning rate\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "learning_rate = tf.train.exponential_decay(\n",
    "    INIT_L_RATE,\n",
    "    global_step,\n",
    "    1,\n",
    "    np.exp(np.log(FINAL_L_RATE/INIT_L_RATE) / NUM_ITER),\n",
    "    staircase=True\n",
    ")\n",
    "\n",
    "# L2 loss function\n",
    "# (we want L_inf error, but use smooth L2 error for optimization instead)\n",
    "##### ##### COMPLETE THE CODE HERE  ##### #####\n",
    "'''\n",
    " Define operations for the loss function. Remember that we will feed batches of input and target vectors\n",
    " to the placeholders but the loss always needs to produce a scalar value independant of the batch size, so make\n",
    " sure your loss operation can handle different batch sizes.\n",
    " \n",
    " We suggest to use the name 'loss' for this operation. If you choose a different name you might\n",
    " have to adjust something in other parts of the code accordingly.\n",
    "'''\n",
    "##### ##### ##### ##### ##### ##### ##### #####\n",
    "\n",
    "# Linf error (we can however still monitor this)\n",
    "error = tf.reduce_max(tf.abs(prediction-target))\n",
    "\n",
    "# use gradient descent during optimization\n",
    "step = tf.train.GradientDescentOptimizer(learning_rate).minimize(\n",
    "    loss,\n",
    "    global_step=global_step\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bookkeeping\n",
    "\n",
    "Let us take a short break and print some information to get an overview of the constructed network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n----------------------------------------------------') \n",
    "print(' RUNNING EXPERIMENT WITH THE FOLLOWING PARAMETERS: ')\n",
    "print('----------------------------------------------------\\n')\n",
    "print('depth:\\t\\t\\t{}'.format(DEPTH))\n",
    "print('width:\\t\\t\\t{}'.format(WIDTH))\n",
    "print('number of neurons:\\t{}'.format(2+(DEPTH-2)*WIDTH))\n",
    "print('number of connections:\\t{}'.format(2+(DEPTH-2)*WIDTH*3+WIDTH*WIDTH*(DEPTH-3)*(DEPTH-2)//2))\n",
    "print('activation:\\t\\t{}'.format(ACTIVATION.__name__))\n",
    "print('learning rate:\\t\\t{} to {}'.format(INIT_L_RATE, FINAL_L_RATE))\n",
    "print('iterations:\\t\\t{}'.format(NUM_ITER))\n",
    "print('batch size:\\t\\t{}'.format(BATCH_SIZE))\n",
    "print('regularity:\\t\\t{}'.format(REGULARITY))\n",
    "print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the training\n",
    "\n",
    "Let us now start the session and run the training steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start Tensorflow session and initialize all network variables\n",
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "# run gradient descent steps\n",
    "print('\\nStarted training...')\n",
    "print('{:8s}\\t{:8s}\\t{:8s}'.format('iter', 'l2-loss', 'linf-err'))\n",
    "print('{:8s}\\t{:8s}\\t{:8s}'.format(*(3*[8*'-'])))\n",
    "for iter in range(NUM_ITER):\n",
    "    # generate random batch of inputs and corresponding target values\n",
    "    input_batch = [DOMAIN[1]-DOMAIN[0], DOMAIN[3]-DOMAIN[2]] \\\n",
    "                  * np.random.rand(BATCH_SIZE, 2) \\\n",
    "                  + [DOMAIN[0], DOMAIN[2]]\n",
    "    target_batch = np.reshape(\n",
    "        target_func(input_batch[:, 0], input_batch[:, 1]),\n",
    "        [-1, 1]\n",
    "    )\n",
    "\n",
    "    # take gradient descent step and compute loss & error\n",
    "    loss_val, error_val, _ = session.run(\n",
    "        [loss, error, step],\n",
    "        feed_dict={input: input_batch, target: target_batch}\n",
    "    )\n",
    "    if iter % 100 == 0:\n",
    "        print('{:8d}\\t{:1.2e}\\t{:1.2e}'.format(iter, loss_val, error_val))\n",
    "print('...finished training.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the trained network\n",
    "\n",
    "After finishing the training phase it is time to check what our network has learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate full sample grid of input domain\n",
    "xrange = np.linspace(DOMAIN[0], DOMAIN[1], num=RESOLUTION)\n",
    "yrange = np.linspace(DOMAIN[2], DOMAIN[3], num=RESOLUTION)\n",
    "xgrid, ygrid = np.meshgrid(xrange, yrange)\n",
    "input_test_batch = np.stack([xgrid.flatten(), ygrid.flatten()], axis=1)\n",
    "\n",
    "# get model predictions\n",
    "prediction_test_batch = np.reshape(\n",
    "    session.run(\n",
    "        prediction,\n",
    "        feed_dict={input: input_test_batch}\n",
    "    ),\n",
    "    xgrid.shape\n",
    ")\n",
    "\n",
    "# get actual target values and compare with predictions\n",
    "target_test_batch = target_func(xgrid, ygrid)\n",
    "l2_err = 1/2*np.mean(np.square(prediction_test_batch-target_test_batch))\n",
    "linf_err = np.max(np.abs(prediction_test_batch-target_test_batch))\n",
    "print(\n",
    "    'Error of predictions after training, evaluated on {}x{} grid:'\n",
    "    .format(RESOLUTION, RESOLUTION)\n",
    ")\n",
    "print('l2:\\t{:1.4e}'.format(l2_err))\n",
    "print('l2inf:\\t{:1.4e}'.format(linf_err))\n",
    "\n",
    "# plot actual target, prediction, and comparison\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(target_test_batch, extent=DOMAIN, origin='lower')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.title('target function')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(prediction_test_batch, extent=DOMAIN, origin='lower')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.title('prediction')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(np.abs(target_test_batch-prediction_test_batch), extent=DOMAIN, origin='lower')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.title('difference')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "\n",
    "Once we are done we can close the session, which will free memory of all our tensors. This is optional, as the session is automatically closed once we close Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
